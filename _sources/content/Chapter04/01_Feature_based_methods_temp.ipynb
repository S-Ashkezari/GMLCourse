{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Supervised Graph Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning has long been a cornerstone of machine learning, where the goal is to learn predictive patterns from labeled data. This framework naturally extends to graph-structured data, where supervision can take various forms—labels may be associated with individual nodes, groups of nodes, or entire graphs. In such cases, the objective becomes learning a function that accurately maps graph-based inputs to their corresponding labels.\n",
    "\n",
    "Graphs are powerful data structures for modeling relational systems, such as social networks, biological interactions, citation networks, and more. Supervised graph learning enables us to harness this structure for tasks like node classification, link prediction, or graph classification. For instance, in a citation graph, we might want to predict the research field of a paper based on its citation neighborhood; in a molecular graph, we may wish to predict the biological activity of a compound.\n",
    "\n",
    "To address these tasks, a variety of supervised graph learning approaches have emerged. These range from traditional feature engineering methods to modern deep learning models like graph neural networks (GNNs), which can learn both structure and features jointly. Throughout this chapter, we will explore key methods and strategies that enable supervised learning on graphs, with an emphasis on embedding techniques and neural architectures designed to capture complex patterns in structured data.\n",
    "\n",
    "The topics covered will include:\n",
    "\n",
    "Key principles of supervised graph embedding\n",
    "\n",
    "Classical feature-based and shallow embedding methods\n",
    "\n",
    "Techniques incorporating graph regularization\n",
    "\n",
    "Neural network-based models, including graph convolutional networks (GCNs)\n",
    "\n",
    "By the end of this chapter, readers will gain a clear understanding of how supervised learning is adapted to graph data, and the tools available to implement these methods effectively.\n",
    "![supervisedML](../images/chap4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Principles of Supervised Graph Embedding\n",
    "\n",
    "Supervised graph embedding aims to transform graph-structured data into low-dimensional vector representations that are informative for downstream prediction tasks. Unlike unsupervised methods that focus solely on preserving graph topology, supervised embeddings are guided by label information, enabling them to learn representations that are more discriminative for specific tasks.\n",
    "\n",
    "The core principles include:\n",
    "\n",
    "- **Label-Guided Representation Learning**: Embeddings are optimized not only to capture structural relationships in the graph but also to align with label similarities. Nodes or subgraphs with the same label should be mapped closer together in the embedding space.\n",
    "\n",
    "- **Task-Specific Optimization**: The embedding process is tied directly to a predictive task, such as node classification or graph-level prediction. This task supervision steers the learning process to focus on features that are most relevant to the objective.\n",
    "\n",
    "- **Joint Learning of Structure and Features**: Supervised methods often integrate node attributes, edge relationships, and label information in a unified framework, enhancing the ability to capture both local and global graph patterns.\n",
    "\n",
    "- **Generalization and Transferability**: The learned embeddings should generalize well to unseen nodes or graphs and ideally be transferable across related tasks or domains.\n",
    "\n",
    "- **End-to-End Trainability**: Many modern supervised graph embedding models, particularly those based on neural networks, are trained in an end-to-end fashion. This allows for direct optimization of the embedding quality with respect to the task loss (e.g., cross-entropy for classification).\n",
    "\n",
    "These principles form the foundation of most state-of-the-art supervised graph learning algorithms, enabling them to achieve high accuracy and interpretability across a variety of graph-based applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8HAOWyDgPiH"
   },
   "source": [
    "# Feature based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Feature-Based Supervised Graph Learning\n",
    "\n",
    "Before the rise of deep learning on graphs, traditional machine learning approaches dominated supervised graph learning tasks. These methods rely on **hand-engineered features** extracted from graph structures and then apply classical classifiers such as Support Vector Machines (SVM), Decision Trees, or Logistic Regression. This approach is particularly effective when domain knowledge can inform feature design or when datasets are small.\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "The main idea behind feature-based supervised graph learning is to **transform the graph data into a fixed-size feature vector** that can be fed into conventional supervised learning models. These features are designed to capture the structural, topological, or statistical properties of the graph, node, or edge.\n",
    "\n",
    "Depending on the granularity of the prediction task, feature vectors are created for:\n",
    "- **Nodes** (for node classification),\n",
    "- **Edges** (for link prediction),\n",
    "- **Entire graphs** (for graph classification).\n",
    "\n",
    "### Commonly Used Graph Features\n",
    "\n",
    "Some typical features used in this paradigm include:\n",
    "\n",
    "- **Degree**: Number of connections a node has.\n",
    "- **Clustering Coefficient**: Measures how connected a node’s neighbors are.\n",
    "- **Betweenness Centrality**: Measures a node’s role in connecting different parts of the graph.\n",
    "- **PageRank**: A variant of eigenvector centrality, used to identify influential nodes.\n",
    "- **Shortest Path Statistics**: Features based on distances to other nodes.\n",
    "- **Neighborhood Subgraph Patterns**: Patterns in the local neighborhood of a node.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "The workflow typically involves the following steps:\n",
    "\n",
    "1. **Feature Extraction**: Compute graph-based features using tools like NetworkX or custom algorithms.\n",
    "2. **Label Assignment**: Use existing labels for nodes/edges/graphs for supervised training.\n",
    "3. **Model Training**: Apply a standard ML algorithm (e.g., Random Forest, SVM) on the feature vectors and labels.\n",
    "4. **Evaluation**: Assess the model on a separate test set using accuracy, F1-score, AUC, etc.\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement.\n",
    "- Compatible with any standard ML pipeline.\n",
    "- Effective when features are well-engineered and meaningful.\n",
    "\n",
    "**Limitations:**\n",
    "- Hand-crafting features can be labor-intensive and domain-specific.\n",
    "- May fail to capture complex dependencies in large and dynamic graphs.\n",
    "- Not scalable to very large graphs or graphs with high variability in structure.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Classical feature-based supervised graph learning provides a foundational approach for tasks involving graph-structured data. Though gradually replaced by automated representation learning methods (e.g., Graph Neural Networks), it remains valuable for its interpretability and simplicity, especially in low-resource settings or as a baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Node Classification in a Citation Network\n",
    "\n",
    "Imagine a citation network where each node represents a scientific paper, and an edge exists between two nodes if one paper cites the other. Each paper belongs to one of several research areas (e.g., AI, systems, theory), and the goal is to **predict the research area** of a paper based on its graph structure.\n",
    "\n",
    "#### Step 1: Feature Extraction\n",
    "We compute several features for each paper (node):\n",
    "- **Degree**: Number of citations.\n",
    "- **Clustering Coefficient**: How tightly connected the paper’s neighbors are.\n",
    "- **PageRank Score**: Importance based on citation structure.\n",
    "- **Average Degree of Neighbors**: Captures neighborhood richness.\n",
    "\n",
    "Using a library like NetworkX in Python:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_networkx\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load Cora dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Planetoid(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Planetoid\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCora\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to NetworkX graph (undirected for clustering)\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Compute graph-based features\n",
    "degree = dict(G.degree())\n",
    "clustering = nx.clustering(G)\n",
    "pagerank = nx.pagerank(G)\n",
    "avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "features = pd.DataFrame({\n",
    "    'degree': pd.Series(degree),\n",
    "    'clustering': pd.Series(clustering),\n",
    "    'pagerank': pd.Series(pagerank),\n",
    "    'avg_neighbor_degree': pd.Series(avg_neighbor_degree)\n",
    "})\n",
    "\n",
    "print(features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_id': 0, 'degree': 3, 'clustering': 0.3333333333333333, 'pagerank': 0.0003356029716229603, 'avg_neighbor_degree': 3.3333333333333335}\n",
      "{'node_id': 1, 'degree': 3, 'clustering': 0, 'pagerank': 0.00038527094424122357, 'avg_neighbor_degree': 2.6666666666666665}\n",
      "{'node_id': 2, 'degree': 5, 'clustering': 0, 'pagerank': 0.0005146369200521648, 'avg_neighbor_degree': 16.0}\n",
      "{'node_id': 3, 'degree': 1, 'clustering': 0, 'pagerank': 0.00036927621861152144, 'avg_neighbor_degree': 1.0}\n",
      "{'node_id': 4, 'degree': 5, 'clustering': 0.7, 'pagerank': 0.000396058502986215, 'avg_neighbor_degree': 6.6}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to a NetworkX graph (undirected for clustering)\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Compute node-level features\n",
    "degree = dict(G.degree())\n",
    "clustering = nx.clustering(G)\n",
    "pagerank = nx.pagerank(G)\n",
    "avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
    "\n",
    "# Combine features manually into a list of dictionaries\n",
    "node_features = []\n",
    "for node in G.nodes():\n",
    "    feature_dict = {\n",
    "        'node_id': node,\n",
    "        'degree': degree.get(node, 0),\n",
    "        'clustering': clustering.get(node, 0.0),\n",
    "        'pagerank': pagerank.get(node, 0.0),\n",
    "        'avg_neighbor_degree': avg_neighbor_degree.get(node, 0.0)\n",
    "    }\n",
    "    node_features.append(feature_dict)\n",
    "\n",
    "# Print first 5 feature dictionaries\n",
    "for i in range(5):\n",
    "    print(node_features[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Supervised Learning\n",
    "Assume we have ground-truth labels for a subset of the papers indicating their field. We train a classifier using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.21      0.21       105\n",
      "           1       0.52      0.35      0.42        65\n",
      "           2       0.54      0.48      0.50       126\n",
      "           3       0.44      0.50      0.47       246\n",
      "           4       0.29      0.31      0.30       128\n",
      "           5       0.20      0.21      0.21        89\n",
      "           6       0.16      0.13      0.14        54\n",
      "\n",
      "    accuracy                           0.36       813\n",
      "   macro avg       0.34      0.31      0.32       813\n",
      "weighted avg       0.37      0.36      0.36       813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert list of dicts to numpy array (features)\n",
    "X = np.array([\n",
    "    [\n",
    "        node['degree'],\n",
    "        node['clustering'],\n",
    "        node['pagerank'],\n",
    "        node['avg_neighbor_degree']\n",
    "    ]\n",
    "    for node in node_features\n",
    "])\n",
    "\n",
    "# Get labels from PyG data\n",
    "y = data.y.cpu().numpy()  # class labels for each paper\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Interpretation\n",
    "The trained model learns how graph-based features (like citation degree or clustering) correlate with a paper’s field. For example, papers in \"theory\" may cite fewer but more central works, while \"systems\" papers might form densely connected communities.\n",
    "\n",
    "This method is simple, interpretable, and effective when the features are meaningful — making it a solid baseline before applying more complex methods like Graph Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPz5JRYvXqH50rX95tddtHr",
   "collapsed_sections": [],
   "name": "Supervised_GraphML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (mlgBookPro)",
   "language": "python",
   "name": "mlgbookpro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Shallow Embedding Methods &#8212; Graph Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/Chapter03/01_Shallow_Embeddings';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="AutoEncoder" href="02_Autoencoders.html" />
    <link rel="prev" title="Chapter 3 : Unsupervised Graph learning" href="00_Unsupervised_Learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Graph Machine Learning - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Graph Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to Graph Machine Learning Workshop
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter00/chap01_intro_and_basics.html">Introduction to Graph Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter00/help.html">Step-by-Step Setup Guide: Anaconda, Virtual Environments, and Jupyter Kernel Configuration</a></li>









</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter01/01_Introduction_Networkx.html">Chapter 1 : Introduction to Networkx</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter01/02_Graph_metrics.html">Chapter 1.2: Graph properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter01/03_Graphs_Benchmarks.html">Benchmark and Repositories</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../Chapter02/01_embedding_examples.html">Chapter 2 : Embedding Examples</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00_Unsupervised_Learning.html">Chapter 3 : Unsupervised Graph learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Shallow Embedding Methods</a></li>


<li class="toctree-l2"><a class="reference internal" href="02_Autoencoders.html">AutoEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_Structural_deep_neural_embeddings.html">Structural Deep Network Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_Graph_Neural_Network.html">üß† Unsupervised Graph Representation Learning using GCN (Graph Convolutional Network)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../Chapter04/01_Feature_based_methods.html">Chapter 4 : Supervised Graph Learning</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/executablebooks/jupyter-book/blob/master/docs/content/Chapter03/01_Shallow_Embeddings.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontent/Chapter03/01_Shallow_Embeddings.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/Chapter03/01_Shallow_Embeddings.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Shallow Embedding Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Shallow Embedding Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-factorization">Matrix Factorization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-factorization">Graph Factorization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-representation-with-global-structure-information-graphrep">Graph Representation with Global Structure Information (GraphRep)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-and-setup">Notation and Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-k-th-order-proximity">What is the <span class="math notranslate nohighlight">\( k \)</span>th-Order Proximity?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#small-graph-example-global-structure-with-graphrep">Small Graph Example: Global Structure with GraphRep</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-structure">üîπ Graph Structure</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-adjacency-matrix-a">üîπ Step 1: Adjacency Matrix <span class="math notranslate nohighlight">\( A \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-degree-matrix-d">üîπ Step 2: Degree Matrix <span class="math notranslate nohighlight">\( D \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-compute-d-1-a">üîπ Step 3: Compute <span class="math notranslate nohighlight">\( D^{-1} A \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">üîπ Interpretation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-compute-d-1-a-2">üîπ Step 4: Compute <span class="math notranslate nohighlight">\( (D^{-1}A)^2 \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">üîπ Interpretation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-left-singular-vectors-y-s-1-truncated-to-top-2">üîπ Step 5: Left Singular Vectors <span class="math notranslate nohighlight">\(  Y_s^{(1)} \)</span> (Truncated to top-2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#y-s-2">üîπ<span class="math notranslate nohighlight">\( Y_s^{(2)} \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-embedding-construction">Final Embedding Construction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-final-embedding-y-s-y-s-1-y-s-2">üîπ step 6: Final Embedding <span class="math notranslate nohighlight">\( Y_s = [Y_s^{(1)} \ \| \ Y_s^{(2)}] \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#higher-order-proximity-preserved-embedding-hope">Higher-Order Proximity Preserved Embedding ( HOPE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Final Embedding Construction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-matrix-computation">Similarity Matrix Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#katz-similarity-in-hope">Katz Similarity in HOPE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-definition">üî∏ Mathematical Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">üî∏ Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#role-in-hope">üî∏ Role in HOPE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-beta">üî∏ Choosing <span class="math notranslate nohighlight">\( \beta \)</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram-model">Skip-Gram Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">Model Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepwalk-learning-node-embeddings-via-random-walks-and-skip-gram">DeepWalk: Learning Node Embeddings via Random Walks and Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understanding-random-walks">üîÅ Step 1: Understanding Random Walks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-applying-skip-gram-to-graph-walks">üß† Step 2: Applying Skip-Gram to Graph Walks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-extracting-node-embeddings">üì¶ Step 3: Extracting Node Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-the-deepwalk-algorithm">üß≠ Summary of the DeepWalk Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">üìå Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram-with-negative-sampling-sgns">üåê Skip-Gram with Negative Sampling (SGNS)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-strategy">üîß Training Strategy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">üß† Loss Function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">üöÄ Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#used-in">üìå Used in:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-difference-of-node2vec-from-deepwalk">üîë Key Difference of Node2Vec from DeepWalk</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#biased-random-walks-exploration-vs-exploitation">üîÑ Biased Random Walks: Exploration vs. Exploitation</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-node2vec-learns-embeddings">üß† How Node2Vec Learns Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-embedding-space-and-visualization">üìä Example: Embedding Space and Visualization</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">üìå Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#edge2vec-learning-edge-representations-from-node-embeddings">Edge2Vec: Learning Edge Representations from Node Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-idea">üí° Core Idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-edge-embedding-operators">üßÆ Common Edge Embedding Operators</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">üß† Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">‚úÖ Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph2vec-learning-vector-representations-of-entire-graphs">Graph2Vec: Learning Vector Representations of Entire Graphs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective">üì¶ Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underlying-concept">üß© Underlying Concept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-steps">ü™ú Algorithm Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">üìå Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="shallow-embedding-methods">
<h1>Shallow Embedding Methods<a class="headerlink" href="#shallow-embedding-methods" title="Link to this heading">#</a></h1>
<p>Shallow embedding methods refer to a category of algorithms that generate fixed, static vector representations for the input data, typically <strong>without involving deep learning architectures or dynamic updates</strong> during downstream tasks.</p>
<section id="matrix-factorization">
<h2>Matrix Factorization<a class="headerlink" href="#matrix-factorization" title="Link to this heading">#</a></h2>
<p>Matrix factorization is a broadly applicable decomposition technique used across various fields. In the context of graph embedding, it serves as a foundational method for deriving low-dimensional node representations from a graph‚Äôs structure.</p>
<p>Several graph embedding techniques leverage matrix factorization to capture structural properties. In this section, we introduce two prominent algorithms:</p>
<ul class="simple">
<li><p><strong>Graph Factorization (GF)</strong></p></li>
<li><p><strong>Higher-Order Proximity preserved Embedding (HOPE)</strong></p></li>
</ul>
<p>Both approaches use matrix factorization to compute node embeddings that preserve different aspects of the graph‚Äôs topology.</p>
<p>Let <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{m \times n}\)</span> represent the input data matrix. Matrix factorization aims to approximate this matrix as the product of two lower-dimensional matrices:<br />
<span class="math notranslate nohighlight">\(W \approx V \times H\)</span><br />
where <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(H \in \mathbb{R}^{d \times n}\)</span> are referred to as the <strong>source</strong> and <strong>abundance</strong> matrices, respectively. The parameter <span class="math notranslate nohighlight">\(d\)</span> indicates the dimensionality of the embedding space.</p>
<p>The matrix factorization process involves learning <span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(H\)</span> by minimizing a loss function, typically defined as the reconstruction error. A commonly used form is the Frobenius norm:<br />
<span class="math notranslate nohighlight">\(\left\| W - V \times H \right\|_F^2\)</span></p>
<p>This fundamental approach underlies many unsupervised graph embedding methods. These techniques represent the input graph as a matrix (such as an adjacency or proximity matrix) and decompose it to capture structural relationships. While the core principle is consistent across different models, they differ in how the loss function is defined. Different loss formulations allow the learned embedding space to highlight specific properties of the graph structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">node_names</span><span class="o">=</span><span class="p">{},</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">pos_nodes</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">spring_layout</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos_nodes</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="n">node_size</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">arrowsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    
    <span class="n">pos_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">coords</span> <span class="ow">in</span> <span class="n">pos_nodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">pos_attrs</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">coords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">coords</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.08</span><span class="p">)</span>
        
    <span class="c1">#nx.draw_networkx_labels(G, pos_attrs, font_family=&#39;serif&#39;, font_size=20)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">1.2</span><span class="o">*</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">axis</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()])</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">1.2</span><span class="o">*</span><span class="n">y</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">axis</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="graph-factorization">
<h2>Graph Factorization<a class="headerlink" href="#graph-factorization" title="Link to this heading">#</a></h2>
<p>Graph Factorization (GF) was among the earliest models designed to efficiently compute node embeddings for a given graph. It follows the general matrix factorization principle described earlier, and specifically factorizes the adjacency matrix of the graph.</p>
<p>Formally, let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be the input graph, and let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{|V| \times |V|}\)</span> denote its adjacency matrix. The GF algorithm aims to learn a low-dimensional embedding matrix <span class="math notranslate nohighlight">\(Y \in \mathbb{R}^{|V| \times d}\)</span>, where each row <span class="math notranslate nohighlight">\(Y_{i,:}\)</span> corresponds to the <span class="math notranslate nohighlight">\(d\)</span>-dimensional embedding of node <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>The loss function used in GF is defined as:</p>
<div class="math notranslate nohighlight">
\[
L = \frac{1}{2} \sum_{(i,j)\in E} \left(A_{i,j} - Y_{i,:} Y_{j,:}^\top \right)^2 
+ \frac{\lambda}{2} \sum_i \left\| Y_{i,:} \right\|^2
\]</div>
<p>Here, <span class="math notranslate nohighlight">\((i, j) \in E\)</span> indicates an edge in the graph, and <span class="math notranslate nohighlight">\(\lambda\)</span> is a regularization coefficient that helps prevent overfitting and ensures the stability of the optimization, especially in sparse graphs.</p>
<p>The first term in the loss measures the reconstruction error between the actual adjacency values and the inner product of the corresponding node embeddings. The second term penalizes large embedding values to control model complexity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">barbell_graph</span><span class="p">(</span><span class="n">m1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m2</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a96869e82c11dee5b7a9c9bcaa89cca7b5b998a98c619fe3c5552adad1fafb7c.png" src="../../_images/a96869e82c11dee5b7a9c9bcaa89cca7b5b998a98c619fe3c5552adad1fafb7c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install git+https://github.com/palash1992/GEM.git</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="n">Path</span><span class="p">(</span><span class="s2">&quot;gem/intermediate&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Instead of this old library, we use newer one</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gem.embedding.gf</span><span class="w"> </span><span class="kn">import</span> <span class="n">GraphFactorization</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">barbell_graph</span><span class="p">(</span><span class="n">m1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m2</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">gf</span> <span class="o">=</span> <span class="n">GraphFactorization</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="n">data_set</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">1</span><span class="o">*</span><span class="mi">10</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">regu</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">gf</span><span class="o">.</span><span class="n">learn_embedding</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e71bcc838db2675287acc4bb8826f57ddd4fc8de616f80fadb29f59c89e6ad02.png" src="../../_images/e71bcc838db2675287acc4bb8826f57ddd4fc8de616f80fadb29f59c89e6ad02.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>./gf not found. Reverting to Python implementation. Please compile gf, place node2vec in the path and grant executable permission
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 7.82088083e-04, -8.13404272e-04],
       [ 7.82853745e-04, -8.14869906e-04],
       [ 7.83630437e-04, -8.09787107e-04],
       [ 7.71588297e-04, -8.03600060e-04],
       [ 8.12845122e-04, -7.92013511e-04],
       [ 8.09525900e-04, -8.96718383e-04],
       [ 4.19101437e-04, -1.09232562e-03],
       [ 7.92443001e-04, -1.42524860e-04],
       [ 1.15134702e-03,  2.84541906e-03],
       [ 6.29887762e-03, -7.55370228e-03],
       [ 1.12422107e-02, -2.54097591e-03],
       [ 1.09115124e-02,  2.51381889e-05],
       [ 2.41105226e-03, -1.96114217e-03],
       [-6.75303751e-03,  1.27246267e-03],
       [-7.92204709e-03, -2.21640189e-04],
       [-7.92103649e-03, -2.21360683e-04],
       [-7.92202615e-03, -2.26751505e-04],
       [-7.91009256e-03, -2.20376636e-04],
       [-7.94889981e-03, -1.69912797e-04],
       [-7.76963301e-03, -3.24342590e-04],
       [-8.37176798e-03, -4.62882252e-04],
       [-8.12746950e-03,  3.66006581e-04],
       [-7.22039641e-03, -1.21010017e-03],
       [-8.39119169e-03,  3.40910987e-03]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">gf</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">()[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2049b9f575761d83abffcc261a457383864d2a33675e052fa58ecec2f448c70b.png" src="../../_images/2049b9f575761d83abffcc261a457383864d2a33675e052fa58ecec2f448c70b.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="graph-representation-with-global-structure-information-graphrep">
<h1>Graph Representation with Global Structure Information (GraphRep)<a class="headerlink" href="#graph-representation-with-global-structure-information-graphrep" title="Link to this heading">#</a></h1>
<p>GraphRep is a family of graph embedding methods that preserve <strong>global structural information</strong>. A well-known example is <strong>HOPE (High-Order Proximity preserved Embedding)</strong>, which captures <strong>higher-order node proximity</strong> without requiring embeddings to be symmetric.</p>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>Most basic embedding methods capture only first-order (direct links) or second-order (shared neighbors) proximities. However, <strong>global proximity</strong> (longer-range relationships between nodes) often reveals richer structure. GraphRep methods address this by considering multiple steps (orders) of proximity in the graph.</p>
</section>
<hr class="docutils" />
<section id="notation-and-setup">
<h2>Notation and Setup<a class="headerlink" href="#notation-and-setup" title="Link to this heading">#</a></h2>
<p>Let:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( G = (V, E) \)</span>: input graph with <span class="math notranslate nohighlight">\( |V| \)</span> nodes</p></li>
<li><p><span class="math notranslate nohighlight">\( A \in \mathbb{R}^{|V| \times |V|} \)</span>: adjacency matrix</p></li>
<li><p><span class="math notranslate nohighlight">\( D \)</span>: diagonal <strong>degree matrix</strong>, defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  D_{ij} =
  \begin{cases}
  \sum_p A_{ip}, &amp; \text{if } i = j \\
  0, &amp; \text{otherwise}
  \end{cases}
  \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\( X^k \in \mathbb{R}^{|V| \times |V| }\)</span>: the <strong>k-th order proximity matrix</strong></p></li>
<li><p><span class="math notranslate nohighlight">\( Y_s^k, Y_t^k \in \mathbb{R}^{|V| \times d} \)</span>: embedding matrices for source and target nodes</p></li>
<li><p><span class="math notranslate nohighlight">\( d \)</span>: dimension of embeddings per proximity order</p></li>
<li><p><span class="math notranslate nohighlight">\( K \)</span>: maximum order of proximity</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-the-k-th-order-proximity">
<h2>What is the <span class="math notranslate nohighlight">\( k \)</span>th-Order Proximity?<a class="headerlink" href="#what-is-the-k-th-order-proximity" title="Link to this heading">#</a></h2>
<p>The <strong><span class="math notranslate nohighlight">\( k \)</span>th-order proximity</strong> measures how strongly two nodes are connected by walks of length <span class="math notranslate nohighlight">\( k \)</span>. It is calculated using the <strong>transition probability matrix</strong>:</p>
<ul class="simple">
<li><p>First-order transition matrix:
<span class="math notranslate nohighlight">\(
X^1 = D^{-1} A
\)</span>
where <span class="math notranslate nohighlight">\( X^1_{ij} \)</span> gives the probability of moving from node <span class="math notranslate nohighlight">\( v_i \)</span> to <span class="math notranslate nohighlight">\( v_j \)</span> in one step.</p></li>
<li><p>Higher-order transition matrix:
<span class="math notranslate nohighlight">\(
X^k = (D^{-1} A)^k
\)</span>
where <span class="math notranslate nohighlight">\( X^k_{ij} \)</span> is the probability of moving from <span class="math notranslate nohighlight">\( v_i \)</span> to <span class="math notranslate nohighlight">\( v_j \)</span> in exactly <span class="math notranslate nohighlight">\( k \)</span> steps.</p></li>
</ul>
<hr class="docutils" />
<section id="small-graph-example-global-structure-with-graphrep">
<h3>Small Graph Example: Global Structure with GraphRep<a class="headerlink" href="#small-graph-example-global-structure-with-graphrep" title="Link to this heading">#</a></h3>
<section id="graph-structure">
<h4>üîπ Graph Structure<a class="headerlink" href="#graph-structure" title="Link to this heading">#</a></h4>
<p>We define a simple undirected graph with 4 nodes:
Graph:</p>
<p>3‚Äì1 ‚Äì 2‚Äì4</p>
<p>Edges: (1-2), (1-3), (2-4)</p>
</section>
<section id="step-1-adjacency-matrix-a">
<h4>üîπ Step 1: Adjacency Matrix <span class="math notranslate nohighlight">\( A \)</span><a class="headerlink" href="#step-1-adjacency-matrix-a" title="Link to this heading">#</a></h4>
<p>The adjacency matrix <span class="math notranslate nohighlight">\( A \in \mathbb{R}^{4 \times 4} \)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = 
\begin{bmatrix}
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="step-2-degree-matrix-d">
<h4>üîπ Step 2: Degree Matrix <span class="math notranslate nohighlight">\( D \)</span><a class="headerlink" href="#step-2-degree-matrix-d" title="Link to this heading">#</a></h4>
<p>The degree matrix $<span class="math notranslate nohighlight">\(D \)</span>$ is diagonal with degrees of each node:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D =
\begin{bmatrix}
2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="step-3-compute-d-1-a">
<h4>üîπ Step 3: Compute <span class="math notranslate nohighlight">\( D^{-1} A \)</span><a class="headerlink" href="#step-3-compute-d-1-a" title="Link to this heading">#</a></h4>
<p>The inverse degree matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D^{-1} =
\begin{bmatrix}
\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \frac{1}{2} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}
\end{split}\]</div>
<p>Now compute:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D^{-1} A =
\begin{bmatrix}
0 &amp; 0.5 &amp; 0.5 &amp; 0 \\
0.5 &amp; 0 &amp; 0 &amp; 0.5 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="interpretation">
<h4>üîπ Interpretation<a class="headerlink" href="#interpretation" title="Link to this heading">#</a></h4>
<p>Each row of <span class="math notranslate nohighlight">\(D^{-1}A\)</span> represents <strong>transition probabilities</strong> from one node to its neighbors.</p>
<ul class="simple">
<li><p>Row 1: From node 1 ‚Üí 50% to node 2, 50% to node 3.</p></li>
<li><p>Row 2: From node 2 ‚Üí 50% to node 1, 50% to node 4.</p></li>
<li><p>Row 3: From node 3 ‚Üí 100% to node 1.</p></li>
<li><p>Row 4: From node 4 ‚Üí 100% to node 2.</p></li>
</ul>
<p>This matrix is used in random walk, label propagation, and spectral methods such as <strong>GraphRep</strong>, capturing the <strong>global structure</strong> of the graph for embedding or factorization.</p>
</section>
<section id="step-4-compute-d-1-a-2">
<h4>üîπ Step 4: Compute <span class="math notranslate nohighlight">\( (D^{-1}A)^2 \)</span><a class="headerlink" href="#step-4-compute-d-1-a-2" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
(D^{-1}A)^2 = (D^{-1}A) \cdot (D^{-1}A)
\]</div>
<p>We perform matrix multiplication:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(D^{-1}A)^2 =
\begin{bmatrix}
0.75 &amp; 0 &amp; 0 &amp; 0.25 \\
0 &amp; 0.75 &amp; 0.25 &amp; 0 \\
0 &amp; 0.5 &amp; 0.5 &amp; 0 \\
0.5&amp; 0 &amp;0 &amp; 0.5 \\
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="id1">
<h4>üîπ Interpretation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>Each entry <span class="math notranslate nohighlight">\( (D^{-1}A)^2_{ij} \)</span> gives the probability of reaching node <span class="math notranslate nohighlight">\( j \)</span> from node <span class="math notranslate nohighlight">\( i \)</span> in <strong>2 steps</strong> under the random walk defined by <span class="math notranslate nohighlight">\( D^{-1}A \)</span>.</p>
<p>Examples:</p>
<ul class="simple">
<li><p>From node 1:</p>
<ul>
<li><p>75% chance to return to itself after 2 steps.</p></li>
<li><p>25% chance to reach node 4 in 2 steps.</p></li>
</ul>
</li>
<li><p>From node 3:</p>
<ul>
<li><p>50% chance to reach node 2.</p></li>
<li><p>50% chance to return to node 3.</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="objective-function">
<h2>Objective Function<a class="headerlink" href="#objective-function" title="Link to this heading">#</a></h2>
<p>For each proximity order <span class="math notranslate nohighlight">\( k \)</span>, the loss function is defined as:</p>
<p><span class="math notranslate nohighlight">\(
L_k = \left\| X^k - Y_s^k \cdot \left(Y_t^k\right)^{T} \right\|_F^2 \quad \text{for } 1 \leq k \leq K
\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \|\cdot\|_F \)</span> is the <strong>Frobenius norm</strong></p></li>
<li><p>This loss measures how well the embedding matrices approximate the proximity matrix <span class="math notranslate nohighlight">\( X^k \)</span></p></li>
</ul>
<p>Each <span class="math notranslate nohighlight">\( L_k \)</span> is minimized <strong>independently</strong> to learn the embedding matrices <span class="math notranslate nohighlight">\( Y_s^k \)</span> and <span class="math notranslate nohighlight">\( Y_t^k \)</span>.</p>
<hr class="docutils" />
<p>In our example we have</p>
<section id="step-5-left-singular-vectors-y-s-1-truncated-to-top-2">
<h3>üîπ Step 5: Left Singular Vectors <span class="math notranslate nohighlight">\(  Y_s^{(1)} \)</span> (Truncated to top-2)<a class="headerlink" href="#step-5-left-singular-vectors-y-s-1-truncated-to-top-2" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
Y_s^{(1)} =
\begin{bmatrix}
\phantom{-}0.0000 &amp; \phantom{-}0.5623 \\
-0.5623 &amp; \phantom{-}0.0000 \\
-0.9099 &amp; -0.0000 \\
\phantom{-}0.0000 &amp; \phantom{-}0.9099 \\
\end{bmatrix}
\end{split}\]</div>
</section>
<hr class="docutils" />
<section id="y-s-2">
<h3>üîπ<span class="math notranslate nohighlight">\( Y_s^{(2)} \)</span><a class="headerlink" href="#y-s-2" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
Y_s^{(2)} =
\begin{bmatrix}
-0.7618 &amp; \phantom{-}0.0000 \\
\phantom{-}0.0000 &amp; -0.7618 \\
\phantom{-}0.0000 &amp; -0.6725 \\
-0.6725 &amp; \phantom{-}0.0000 \\
\end{bmatrix}
\end{split}\]</div>
</section>
</section>
<hr class="docutils" />
<section id="final-embedding-construction">
<h2>Final Embedding Construction<a class="headerlink" href="#final-embedding-construction" title="Link to this heading">#</a></h2>
<p>After optimization, the final embedding for each node is constructed by <strong>concatenating</strong> the source embeddings from all orders:</p>
<p><span class="math notranslate nohighlight">\(
Y_s = \left[ Y_s^1 \ \| \ Y_s^2 \ \| \ \cdots \ \| \ Y_s^K \right] \in \mathbb{R}^{|V| \times (d \cdot K)}
\)</span></p>
<ul class="simple">
<li><p><strong>Number of rows</strong>: <span class="math notranslate nohighlight">\( |V| \)</span> (one per node)</p></li>
<li><p><strong>Number of columns</strong>: <span class="math notranslate nohighlight">\( d \cdot K\)</span></p></li>
</ul>
<p>This way, each node has a composite embedding that captures multi-step structural information.</p>
</section>
<hr class="docutils" />
<section id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dimension</span></code>: the embedding dimension per proximity order (<span class="math notranslate nohighlight">\( d \)</span>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">order</span></code>: the maximum number of proximity steps considered (<span class="math notranslate nohighlight">\(K \)</span>)</p></li>
<li><p>Final embedding dimension per node: <span class="math notranslate nohighlight">\( d \times K \)</span></p></li>
</ul>
<hr class="docutils" />
<hr class="docutils" />
<section id="step-6-final-embedding-y-s-y-s-1-y-s-2">
<h3>üîπ step 6: Final Embedding <span class="math notranslate nohighlight">\( Y_s = [Y_s^{(1)} \ \| \ Y_s^{(2)}] \)</span><a class="headerlink" href="#step-6-final-embedding-y-s-y-s-1-y-s-2" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
Y_s =
\begin{bmatrix}
\phantom{-}0.0000 &amp; \phantom{-}0.5623 &amp; -0.7618 &amp; \phantom{-}0.0000 \\
-0.5623 &amp; \phantom{-}0.0000 &amp; \phantom{-}0.0000 &amp; -0.7618 \\
-0.9099 &amp; \phantom{-}0.0000 &amp; \phantom{-}0.0000 &amp; -0.6725 \\
\phantom{-}0.0000 &amp; \phantom{-}0.9099 &amp; -0.6725 &amp; \phantom{-}0.0000 \\
\end{bmatrix}
\end{split}\]</div>
<p>Each row corresponds to the low-dimensional representation of a node based on global structure information up to 2 hops.</p>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>GraphRep methods (like HOPE):</p>
<ul class="simple">
<li><p>Capture <strong>high-order proximity</strong> and <strong>global graph structure</strong></p></li>
<li><p>Use <strong>random walk transition matrices</strong> up to <span class="math notranslate nohighlight">\( K \)</span> steps</p></li>
<li><p>Allow for <strong>asymmetric embeddings</strong> in directed graphs</p></li>
<li><p>Construct embeddings via <strong>matrix factorization</strong></p></li>
<li><p>Combine embeddings from each proximity order to form the final representation</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Code of the example</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Step 1: Define the adjacency matrix A for the small graph</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># Step 2: Compute the degree matrix D and its inverse</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">D_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="c1"># Step 3: Compute D^-1 * A (normalized adjacency matrix)</span>
<span class="n">DA</span> <span class="o">=</span> <span class="n">D_inv</span> <span class="o">@</span> <span class="n">A</span>

<span class="c1"># Step 4: Compute (D^-1 * A)^2</span>
<span class="n">DA2</span> <span class="o">=</span> <span class="n">DA</span> <span class="o">@</span> <span class="n">DA</span>

<span class="c1"># Step 5: Choose embedding dimension (e.g., k = 2)</span>
<span class="c1"># We use SVD on DA2 (which captures 2-hop structure)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">DA2</span><span class="p">)</span>

<span class="c1"># Truncate to top-k components for embedding</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Y_s</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
<span class="n">Y_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span><span class="p">[:</span><span class="n">k</span><span class="p">]))</span> <span class="o">@</span> <span class="n">Vt</span><span class="p">[:</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span>


<span class="c1"># Final node embeddings (optional: use Y_s directly or combine Y_s and Y_t)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">Y_s</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span><span class="p">[:</span><span class="n">k</span><span class="p">]))</span>



<span class="c1"># Step 1: Define a function to compute Y_s for a given power k</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_graphrep_embedding</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">D_inv</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="c1"># Compute (D^-1 A)^k</span>
    <span class="n">DA</span> <span class="o">=</span> <span class="n">D_inv</span> <span class="o">@</span> <span class="n">A</span>
    <span class="n">DA_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">DA</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Apply SVD</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">DA_k</span><span class="p">)</span>
    
    <span class="c1"># Truncate to top-d components</span>
    <span class="n">Y_s_k</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span><span class="p">[:</span><span class="n">d</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">Y_s_k</span>

<span class="c1"># Parameters</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>   <span class="c1"># embedding dimension per scale</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>   <span class="c1"># maximum hop</span>

<span class="c1"># Compute D^-1</span>
<span class="n">D_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Compute Y_s^1 and Y_s^2</span>
<span class="n">Y_s_1</span> <span class="o">=</span> <span class="n">compute_graphrep_embedding</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">D_inv</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
<span class="n">Y_s_2</span> <span class="o">=</span> <span class="n">compute_graphrep_embedding</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">D_inv</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>

<span class="c1"># Final embedding: concatenate along the feature axis</span>
<span class="n">Y_final</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">Y_s_1</span><span class="p">,</span> <span class="n">Y_s_2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Y_s_1</span><span class="p">,</span> <span class="n">Y_s_2</span><span class="p">,</span> <span class="n">Y_final</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[ 1.18753536e-16,  5.62341325e-01],
        [-5.62341325e-01,  8.90651522e-17],
        [-9.09887377e-01, -5.93767681e-17],
        [ 0.00000000e+00,  9.09887377e-01]]),
 array([[-0.76182285,  0.        ],
        [ 0.        , -0.76182285],
        [ 0.        , -0.67252367],
        [-0.67252367,  0.        ]]),
 array([[ 1.18753536e-16,  5.62341325e-01, -7.61822854e-01,
          0.00000000e+00],
        [-5.62341325e-01,  8.90651522e-17,  0.00000000e+00,
         -7.61822854e-01],
        [-9.09887377e-01, -5.93767681e-17,  0.00000000e+00,
         -6.72523669e-01],
        [ 0.00000000e+00,  9.09887377e-01, -6.72523669e-01,
          0.00000000e+00]]))
</pre></div>
</div>
</div>
</div>
<p>In this implementation,
the dimension parameter represents the dimension of the embedding space, while the
order parameter defines the maximum number of orders of proximity between nodes.
The number of columns of the final embedding matrix (stored, in the example, in the
embeddings variable) is dimension*order, since, as we said, for each proximity
order an embedding is computed and concatenated in the final embedding matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">karateclub.node_embedding.neighbourhood.grarep</span><span class="w"> </span><span class="kn">import</span> <span class="n">GraRep</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">barbell_graph</span><span class="p">(</span><span class="n">m1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m2</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">gr</span> <span class="o">=</span> <span class="n">GraRep</span><span class="p">(</span><span class="n">dimensions</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">gr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/13f9988b6c1409af856dc7f921de0f1aacd6be5a88b83cdfb1ab791fb158de4b.png" src="../../_images/13f9988b6c1409af856dc7f921de0f1aacd6be5a88b83cdfb1ab791fb158de4b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># Since dimensions=2 and order=3, we have 6 dimensions embedded space.</span>
<span class="n">ida</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">idb</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">()[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">ida</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="n">idb</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">ida</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="n">idb</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/805a7ad0e1bca9d37e841a342a3b37238153538787b43c3efb0ae53445a9a263.png" src="../../_images/805a7ad0e1bca9d37e841a342a3b37238153538787b43c3efb0ae53445a9a263.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="higher-order-proximity-preserved-embedding-hope">
<h2>Higher-Order Proximity Preserved Embedding ( HOPE)<a class="headerlink" href="#higher-order-proximity-preserved-embedding-hope" title="Link to this heading">#</a></h2>
<p>HOPE is another graph embedding technique based on the matrix factorization principle.
This method allows preserving higher-order proximity and does not force its embeddings
to have any symmetric properties.</p>
<p>Given the notion of high-order proximity in graphs, the <strong>HOPE</strong> algorithm is designed to generate node embeddings that preserve these structural relationships, particularly in <strong>directed</strong> graphs where proximity is often asymmetric.</p>
<p>Let the input graph be defined as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(G = (V, E)\)</span>: a graph with node set <span class="math notranslate nohighlight">\(V\)</span> and edge set <span class="math notranslate nohighlight">\(E\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A \in \mathbb{R}^{|V| \times |V|}\)</span>: the adjacency matrix of <span class="math notranslate nohighlight">\(G\)</span></p></li>
</ul>
<p>The goal is to approximate a <strong>similarity matrix</strong> <span class="math notranslate nohighlight">\(S\)</span> using two separate embedding matrices, one for source nodes and another for target nodes. The optimization objective (loss function) is defined as:</p>
<div class="math notranslate nohighlight">
\[
L = \left\| S - Y_s \cdot Y_t^{T} \right\|_F^2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S \in \mathbb{R}^{|V| \times |V|}\)</span> is a similarity matrix derived from graph <span class="math notranslate nohighlight">\(G\)</span> that encodes high-order proximity between nodes</p></li>
<li><p><span class="math notranslate nohighlight">\(Y_s \in \mathbb{R}^{|V| \times d}\)</span>: source embedding matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(Y_t \in \mathbb{R}^{|V| \times d}\)</span>: target embedding matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(d\)</span> is the embedding dimensionality</p></li>
<li><p><span class="math notranslate nohighlight">\(\|\cdot\|_F\)</span> denotes the Frobenius norm</p></li>
</ul>
<p>The use of <strong>separate embeddings for source and target nodes</strong> allows HOPE to effectively capture <strong>asymmetric</strong> relationships in the graph‚Äîcommon in many real-world networks such as citation, hyperlink, or follower graphs.</p>
<section id="id2">
<h3>Final Embedding Construction<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>The final embedding of each node is obtained by <strong>concatenating</strong> its source and target embeddings:</p>
<div class="math notranslate nohighlight">
\[
Y = [Y_s \,|\, Y_t] \in \mathbb{R}^{|V| \times 2d}
\]</div>
<p>This means each node is ultimately represented in a <span class="math notranslate nohighlight">\(2d\)</span>-dimensional space.</p>
</section>
<hr class="docutils" />
<section id="similarity-matrix-computation">
<h3>Similarity Matrix Computation<a class="headerlink" href="#similarity-matrix-computation" title="Link to this heading">#</a></h3>
<p>The similarity matrix <span class="math notranslate nohighlight">\(S\)</span> is designed to reflect higher-order proximities between nodes and is computed as:</p>
<div class="math notranslate nohighlight">
\[
S = M_g \cdot M_l
\]</div>
<p>where both <span class="math notranslate nohighlight">\(M_g\)</span> and <span class="math notranslate nohighlight">\(M_l\)</span> are matrix polynomials chosen based on the desired similarity measure. For example, when using the <strong>Adamic-Adar (AA)</strong> similarity measure, we have:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(M_g = I\)</span> (identity matrix)</p></li>
<li><p><span class="math notranslate nohighlight">\(M_l = A D A\)</span>, where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(A\)</span> is the adjacency matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is a diagonal matrix defined as:</p>
<div class="math notranslate nohighlight">
\[
    D_{ii} = \frac{1}{\sum_j (A_{ij} + A_{ji})}
    \]</div>
</li>
</ul>
</li>
</ul>
<p>This formulation highlights the importance of shared neighbors weighted by node degrees, which is characteristic of the Adamic-Adar method.</p>
</section>
<section id="katz-similarity-in-hope">
<h3>Katz Similarity in HOPE<a class="headerlink" href="#katz-similarity-in-hope" title="Link to this heading">#</a></h3>
<p>The <strong>Katz similarity</strong> is a high-order proximity measure used to quantify the relationship between nodes in a graph. It considers <strong>all paths</strong> between two nodes, while giving <strong>shorter paths more weight</strong> than longer ones.</p>
<section id="mathematical-definition">
<h4>üî∏ Mathematical Definition<a class="headerlink" href="#mathematical-definition" title="Link to this heading">#</a></h4>
<p>Given a graph with adjacency matrix ( A ), the Katz similarity matrix ( S ) is defined as:</p>
<p><span class="math notranslate nohighlight">\(
S = \sum_{k=1}^{\infty} \beta^k A^k = \beta A + \beta^2 A^2 + \beta^3 A^3 + \cdots
\)</span></p>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( A^k \)</span>: counts the number of paths of length ( k ) between nodes,</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta \in (0, 1)\)</span>: a decay factor that ensures the series converges,</p></li>
<li><p><span class="math notranslate nohighlight">\( S_{i,j} \)</span>: similarity score between node ( i ) and node ( j ).</p></li>
</ul>
</section>
<section id="intuition">
<h4>üî∏ Intuition<a class="headerlink" href="#intuition" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The Katz index captures <strong>both direct and indirect relationships</strong> between nodes.</p></li>
<li><p>It assigns higher scores to node pairs that are connected by <strong>many short paths</strong>.</p></li>
<li><p>Longer paths are exponentially down-weighted by the decay factor ( \beta ).</p></li>
</ul>
</section>
<section id="role-in-hope">
<h4>üî∏ Role in HOPE<a class="headerlink" href="#role-in-hope" title="Link to this heading">#</a></h4>
<p>In the HOPE algorithm:</p>
<ul class="simple">
<li><p>The <strong>Katz similarity matrix ( S )</strong> is used to represent node proximities.</p></li>
<li><p>A <strong>generalized SVD</strong> is applied to ( S ) to obtain low-dimensional embeddings.</p></li>
<li><p>The embeddings aim to preserve these high-order proximities in the vector space.</p></li>
</ul>
</section>
<section id="choosing-beta">
<h4>üî∏ Choosing <span class="math notranslate nohighlight">\( \beta \)</span><a class="headerlink" href="#choosing-beta" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \beta\)</span> must be <strong>small enough</strong> to ensure convergence (i.e., <span class="math notranslate nohighlight">\(\beta &lt; \frac{1}{\lambda_{\text{max}}} \)</span>, where <span class="math notranslate nohighlight">\( \lambda_{\text{max}} \)</span> is the largest eigenvalue of <span class="math notranslate nohighlight">\( A \)</span>).</p></li>
<li><p>A typical value: <code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">=</span> <span class="pre">0.01</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gem.embedding.hope</span><span class="w"> </span><span class="kn">import</span> <span class="n">HOPE</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">barbell_graph</span><span class="p">(</span><span class="n">m1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m2</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">hp</span> <span class="o">=</span> <span class="n">HOPE</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">hp</span><span class="o">.</span><span class="n">learn_embedding</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/58b6d0827406a984ebcc2b65a60419714c40a0730f057c39387922b748650559.png" src="../../_images/58b6d0827406a984ebcc2b65a60419714c40a0730f057c39387922b748650559.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SVD error (low rank): 0.052092
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],
       [-0.07104037, -0.07104201, -0.07104037, -0.07104201],
       [-0.00797181, -0.00799433, -0.00797181, -0.00799433],
       [-0.00079628, -0.00099787, -0.00079628, -0.00099787],
       [ 0.00079628, -0.00099787,  0.00079628, -0.00099787],
       [ 0.00797181, -0.00799433,  0.00797181, -0.00799433],
       [ 0.07104037, -0.07104201,  0.07104037, -0.07104201],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],
       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">hp</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">()[</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">:]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6fac15099deec2f17a16511ceb68a5af0113e4322fece6a53bdae274758a7daa.png" src="../../_images/6fac15099deec2f17a16511ceb68a5af0113e4322fece6a53bdae274758a7daa.png" />
</div>
</div>
<p>In this case, the graph is undirected and thus there is no difference between the source
and target nodes. Figure shows the first two dimensions of the embeddings matrix
representing . It is possible to see how the embedding space generated by HOPE
provides, in this case, a better separation of the different nodes.</p>
<hr class="docutils" />
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="skip-gram-model">
<h1>Skip-Gram Model<a class="headerlink" href="#skip-gram-model" title="Link to this heading">#</a></h1>
<p>The <strong>Skip-Gram</strong> model is a shallow neural network with a single hidden layer, designed to learn vector representations (embeddings) of words based on their context in a corpus. Its primary objective is to predict the surrounding words (context) of a given input word (target).</p>
<p>The training process begins by constructing training samples from a large text corpus. For each word in the corpus, a fixed-size context window of width <span class="math notranslate nohighlight">\( w\)</span> is defined around it. The target word is the central word in the window, and the words within the window are considered its context. For example, for a sentence like:</p>
<blockquote>
<div><p>‚ÄúThe cat sits on the mat‚Äù</p>
<p>with <span class="math notranslate nohighlight">\( w = 2 \)</span>, the target word ‚Äúsits‚Äù will have the context words: ‚ÄúThe‚Äù, ‚Äúcat‚Äù, ‚Äúon‚Äù, and ‚Äúthe‚Äù.</p>
</div></blockquote>
<p>This process generates multiple (target, context) word pairs, such as (‚Äúsits‚Äù, ‚Äúcat‚Äù), (‚Äúsits‚Äù, ‚Äúon‚Äù), etc. These pairs form the basis of the training data.</p>
<p>The Skip-Gram model is then trained to <strong>maximize the probability</strong> of observing a context word given a target word. More formally, for a given target word <span class="math notranslate nohighlight">\( w_t \)</span>, the model learns to predict the probability of each context word <span class="math notranslate nohighlight">\( w_{c} \)</span> within the window:</p>
<p><span class="math notranslate nohighlight">\(
P(w_{c} \mid w_{t})
\)</span></p>
<p><img alt="image.png" src="content/Chapter03/attachment:6a15b260-825d-4217-989c-e412c262d468.png" /></p>
<section id="model-architecture">
<h2>Model Architecture<a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h2>
<p>The architecture typically consists of:</p>
<ul class="simple">
<li><p><strong>Input layer</strong>: one-hot encoding of the target word.</p></li>
<li><p><strong>Hidden layer</strong>: a dense layer of size <span class="math notranslate nohighlight">\( d \)</span>, where <span class="math notranslate nohighlight">\( d\)</span> is the dimensionality of the word vectors.</p></li>
<li><p><strong>Output layer</strong>: softmax over the vocabulary to predict context words.</p></li>
</ul>
<p>During training, the network learns to map words into a continuous vector space such that semantically similar words are close to each other. This vector space is often referred to as <strong>Word Embedding</strong>, and the entire process is a basis for the <strong>Word2Vec</strong> model, where Skip-Gram is one of the two main architectures (the other being Continuous Bag of Words, or CBOW).</p>
<p>The structure of the neural network representing the skip-gram model is described in the
following chart:</p>
<p><img alt="{7BE2F5C6-4AED-40EC-A413-A4709712B7AB}.png" src="content/Chapter03/attachment:80ac2be5-aece-4257-a23b-6fb438433fb4.png" /></p>
<p>The input of the neural network is a binary vector of size <span class="math notranslate nohighlight">\(m\)</span>. Each element of the vector represents a word in the dictionary of the language we want to embed the words in. When, during the training process, a (target word, context word) pair is given, the input array will have 0 in all its entries with the exception of the entry representing the ‚Äútarget‚Äù word, which will be equal to 1. The hidden layer has d neurons. The hidden layer will learn the embedding representation of each word, creating a d-dimensional embedding space. Finally, the output layer of the neural network is a dense layer of <span class="math notranslate nohighlight">\(m\)</span> neurons (the same size as the input vector) with a softmax activation function. Each neuron represents a word of the dictionary. The value assigned by the neuron corresponds to the probability of that word being ‚Äúrelated‚Äù to the input word. Since softmax can be hard to compute when the
size of <span class="math notranslate nohighlight">\(m\)</span> increases, a hierarchical softmax approach is always used.</p>
<p>The final goal of the skip-gram model is not to actually learn the task we previously described but to build a compact d-dimensional representation of the input words. Thanks to this representation, it is possible to easily extract an embedding space for the words using the weight of the hidden layer. Another common approach to creating a skip-gram model, which will be not described here, is context-based: Continuous Bag-of-Words (CBOW).</p>
<p>When training this neural network, the input is a one-hot encoded vector representing the input word, and the output is also a one-hot encoded vector representing the context word. Remember from the previous image how we constructed input and context pairs of words. Word2vec uses a trick where we aren‚Äôt interested in the output vector of the neural network, but rather the goal is to learn the weights of the hidden layer. The weights of the hidden layer are actually the word embedding we are trying to learn. The number of neurons in the hidden layer will determine the embedding dimension or the size of the vector representing each word in the vocabulary.</p>
<p>Note that the neural network does not consider the offset of the context word, so it does not differentiate between directly adjacent context words to the input and those more distant in the context window or even if the context word precedes or follows the input term. Consequently, the window size parameter has a significant influence on the results of the word embedding. For example, a study Dependency-Based Word Embeddings by Levy &amp; Goldberg finds that larger context window size tends to capture more topic/domain information. In contrast, smaller windows tend to capture more information about the word itself, e.g., what other words are functionally similar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Step 1: Define a sample sentence and extract vocabulary</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;the cat sat on the mat&quot;</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>  <span class="c1"># Unique words in the sentence</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="c1"># Create word-to-index and index-to-word mappings</span>
<span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="c1"># Step 2: Convert a word to a one-hot encoded vector</span>
<span class="k">def</span><span class="w"> </span><span class="nf">word_to_onehot</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="n">vec</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vec</span>

<span class="c1"># Step 3: Generate skip-gram (input, context) pairs with a sliding window</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_skipgram_pairs</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">center_pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">center_pos</span><span class="p">]</span>
        <span class="c1"># Look at surrounding words within the window</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">window_size</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">context_pos</span> <span class="o">=</span> <span class="n">center_pos</span> <span class="o">+</span> <span class="n">w</span>
            <span class="k">if</span> <span class="n">context_pos</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">context_pos</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="ow">or</span> <span class="n">center_pos</span> <span class="o">==</span> <span class="n">context_pos</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">context_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">context_pos</span><span class="p">]</span>
            <span class="n">pairs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">center_word</span><span class="p">,</span> <span class="n">context_word</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pairs</span>

<span class="c1"># Generate training data</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">generate_skipgram_pairs</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Step 4: Print results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:&quot;</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Word to index mapping:&quot;</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Input-Context pairs (Skip-Gram format):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">ctx</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">inp</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ctx</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Example one-hot vectors:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">ctx</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>  <span class="c1"># Just show first 5 examples</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input word:&quot;</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="s2">&quot;‚Üí&quot;</span><span class="p">,</span> <span class="n">word_to_onehot</span><span class="p">(</span><span class="n">inp</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Context word:&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="s2">&quot;‚Üí&quot;</span><span class="p">,</span> <span class="n">word_to_onehot</span><span class="p">(</span><span class="n">ctx</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary: [&#39;cat&#39;, &#39;mat&#39;, &#39;on&#39;, &#39;sat&#39;, &#39;the&#39;]
Word to index mapping: {&#39;cat&#39;: 0, &#39;mat&#39;: 1, &#39;on&#39;: 2, &#39;sat&#39;: 3, &#39;the&#39;: 4}

Input-Context pairs (Skip-Gram format):
(the, cat)
(the, sat)
(cat, the)
(cat, sat)
(cat, on)
(sat, the)
(sat, cat)
(sat, on)
(sat, the)
(on, cat)
(on, sat)
(on, the)
(on, mat)
(the, sat)
(the, on)
(the, mat)
(mat, on)
(mat, the)

Example one-hot vectors:
Input word: the ‚Üí [0. 0. 0. 0. 1.]
Context word: cat ‚Üí [1. 0. 0. 0. 0.]
---
Input word: the ‚Üí [0. 0. 0. 0. 1.]
Context word: sat ‚Üí [0. 0. 0. 1. 0.]
---
Input word: cat ‚Üí [1. 0. 0. 0. 0.]
Context word: the ‚Üí [0. 0. 0. 0. 1.]
---
Input word: cat ‚Üí [1. 0. 0. 0. 0.]
Context word: sat ‚Üí [0. 0. 0. 1. 0.]
---
Input word: cat ‚Üí [1. 0. 0. 0. 0.]
Context word: on ‚Üí [0. 0. 1. 0. 0.]
---
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="deepwalk-learning-node-embeddings-via-random-walks-and-skip-gram">
<h2>DeepWalk: Learning Node Embeddings via Random Walks and Skip-Gram<a class="headerlink" href="#deepwalk-learning-node-embeddings-via-random-walks-and-skip-gram" title="Link to this heading">#</a></h2>
<p>The <strong>DeepWalk</strong> algorithm is a method for learning vector representations (embeddings) of nodes in a graph. It adapts techniques from natural language processing‚Äîparticularly the <strong>skip-gram model</strong>‚Äîto work with graph-structured data.</p>
<hr class="docutils" />
<section id="step-1-understanding-random-walks">
<h3>üîÅ Step 1: Understanding Random Walks<a class="headerlink" href="#step-1-understanding-random-walks" title="Link to this heading">#</a></h3>
<p>To understand DeepWalk, we first need to define the concept of a <strong>random walk</strong>.</p>
<p>Let ( G ) be a graph and ( v_i ) a node selected as the starting point. From ( v_i ), we randomly choose a neighboring node and move to it. From that node, we repeat the process: randomly select a neighbor and move again. This sequence continues for ( t ) steps, resulting in a <strong>random walk of length ( t )</strong>.</p>
<blockquote>
<div><p>Note: There are no constraints on the structure or content of the walk. Therefore, the local neighborhood of a node might not be fully preserved.</p>
</div></blockquote>
<p><img alt="{C30A40F4-DCCE-408C-AD23-2C4C3BFF59DD}.png" src="content/Chapter03/attachment:a313ad89-33e3-45c6-878e-46a27af780df.png" /></p>
</section>
<hr class="docutils" />
<section id="step-2-applying-skip-gram-to-graph-walks">
<h3>üß† Step 2: Applying Skip-Gram to Graph Walks<a class="headerlink" href="#step-2-applying-skip-gram-to-graph-walks" title="Link to this heading">#</a></h3>
<p>Once a set of random walks is generated (typically several per node), DeepWalk treats these walks as <strong>sentences</strong>, where each node is like a <strong>word</strong>.</p>
<p>Just like skip-gram learns word embeddings by predicting nearby words, here we learn <strong>node embeddings</strong> by predicting neighboring nodes in the walk.</p>
<p>Key parameters for training:</p>
<ul class="simple">
<li><p><strong>Window size ( w )</strong>: How many nodes around the target to consider as context</p></li>
<li><p><strong>Embedding dimension ( d )</strong>: Size of the vector representation to be learned</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="step-3-extracting-node-embeddings">
<h3>üì¶ Step 3: Extracting Node Embeddings<a class="headerlink" href="#step-3-extracting-node-embeddings" title="Link to this heading">#</a></h3>
<p>Once training is complete, the weights between the input and hidden layer in the skip-gram neural network represent the <strong>node embeddings</strong>.</p>
<p>These embeddings capture structural information from the graph and encode similarities between nodes based on how frequently they co-occur in random walks.</p>
</section>
<hr class="docutils" />
<section id="summary-of-the-deepwalk-algorithm">
<h3>üß≠ Summary of the DeepWalk Algorithm<a class="headerlink" href="#summary-of-the-deepwalk-algorithm" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Random Walk Generation</strong><br />
For each node in the graph, generate multiple random walks of maximum length ( t ).</p></li>
<li><p><strong>Skip-Gram Training</strong><br />
Train a skip-gram model using the random walks as input, treating each walk as a sentence and each node as a word.</p></li>
<li><p><strong>Embedding Extraction</strong><br />
Use the learned weights of the skip-gram model to extract a low-dimensional vector representation (embedding) for each node.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="applications">
<h3>üìå Applications<a class="headerlink" href="#applications" title="Link to this heading">#</a></h3>
<p>Node embeddings generated by DeepWalk can be used in:</p>
<ul class="simple">
<li><p>Node classification</p></li>
<li><p>Community detection</p></li>
<li><p>Link prediction</p></li>
<li><p>Graph visualization</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">karateclub.node_embedding.neighbourhood.deepwalk</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeepWalk</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">barbell_graph</span><span class="p">(</span><span class="n">m1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m2</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">dw</span> <span class="o">=</span> <span class="n">DeepWalk</span><span class="p">(</span><span class="n">dimensions</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">dw</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0b7dfc5b2b1647bcb59b568ecb76e7d12feba9591c9f14201dece16d053b6671.png" src="../../_images/0b7dfc5b2b1647bcb59b568ecb76e7d12feba9591c9f14201dece16d053b6671.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">dw</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">()[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/077db181388ae0df9757f2ba538bd346f1ad0ad44305adda05448f00d5e61129.png" src="../../_images/077db181388ae0df9757f2ba538bd346f1ad0ad44305adda05448f00d5e61129.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="node2vec">
<h2>Node2Vec<a class="headerlink" href="#node2vec" title="Link to this heading">#</a></h2>
<p><strong>Node2Vec</strong> is an advanced graph embedding algorithm that builds upon the foundation laid by <strong>DeepWalk</strong>.<br />
Like DeepWalk, it learns vector representations (embeddings) for nodes by feeding random walks to a <strong>Skip-Gram model with Negative Sampling (SGNS)</strong>. The learned embeddings capture structural roles and node similarities in the graph.</p>
<section id="skip-gram-with-negative-sampling-sgns">
<h3>üåê Skip-Gram with Negative Sampling (SGNS)<a class="headerlink" href="#skip-gram-with-negative-sampling-sgns" title="Link to this heading">#</a></h3>
<p><strong>SGNS</strong> is a scalable and efficient version of the Skip-Gram model used to learn vector representations (embeddings) for words or nodes.</p>
<p>Instead of using a full softmax over all vocabulary (which is expensive), it uses a simplified objective that only looks at:</p>
<ul class="simple">
<li><p>‚úÖ Positive context pairs (observed together)</p></li>
<li><p>‚ùå Negative samples (randomly selected, assumed unrelated)</p></li>
</ul>
<section id="training-strategy">
<h4>üîß Training Strategy<a class="headerlink" href="#training-strategy" title="Link to this heading">#</a></h4>
<p>For a given input word (or node) <span class="math notranslate nohighlight">\( w_I \)</span> and a real context word <span class="math notranslate nohighlight">\( w_O \)</span>, the model:</p>
<ul class="simple">
<li><p>Maximizes the probability of <span class="math notranslate nohighlight">\( w_O \)</span> being in the context of <span class="math notranslate nohighlight">\( w_I \)</span></p></li>
<li><p>Minimizes the probability of <span class="math notranslate nohighlight">\( k \)</span> randomly selected negative words being in the context</p></li>
</ul>
</section>
<section id="loss-function">
<h4>üß† Loss Function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h4>
<p>The SGNS loss function is defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\log \sigma(\mathbf{v}_{w_O}^\top \mathbf{v}_{w_I}) 
- \sum_{i=1}^{k} \log \sigma(-\mathbf{v}_{w_i^-}^\top \mathbf{v}_{w_I})
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \sigma(x) = \frac{1}{1 + e^{-x}} \)</span>: Sigmoid function</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{v}_{w_I} \)</span>: Input embedding vector</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{v}_{w_O} \)</span>: Output embedding vector of positive context</p></li>
<li><p><span class="math notranslate nohighlight">\( w_i^- \)</span>: Negative sample drawn from a noise distribution (e.g., unigram distribution raised to 3/4)</p></li>
<li><p><span class="math notranslate nohighlight">\( k \)</span>: Number of negative samples</p></li>
</ul>
</section>
<section id="advantages">
<h4>üöÄ Advantages<a class="headerlink" href="#advantages" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>‚ö° <strong>Much faster</strong> than full softmax</p></li>
<li><p>üìà <strong>Scales well</strong> to large vocabularies or graphs</p></li>
<li><p>üß† Produces <strong>meaningful embeddings</strong> (semantic or structural similarity)</p></li>
</ul>
</section>
<section id="used-in">
<h4>üìå Used in:<a class="headerlink" href="#used-in" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">word2vec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DeepWalk</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">node2vec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FastText</span></code></p></li>
</ul>
</section>
<section id="key-difference-of-node2vec-from-deepwalk">
<h4>üîë Key Difference of Node2Vec from DeepWalk<a class="headerlink" href="#key-difference-of-node2vec-from-deepwalk" title="Link to this heading">#</a></h4>
<p>While <strong>DeepWalk</strong> generates random walks using a simple, unbiased strategy (uniformly choosing neighbors), <strong>Node2Vec introduces <em>biased random walks</em></strong>.<br />
These walks blend <strong>Breadth-First Search (BFS)</strong> and <strong>Depth-First Search (DFS)</strong> to control how the graph is explored.</p>
<section id="biased-random-walks-exploration-vs-exploitation">
<h5>üîÑ Biased Random Walks: Exploration vs. Exploitation<a class="headerlink" href="#biased-random-walks-exploration-vs-exploitation" title="Link to this heading">#</a></h5>
<p>Node2Vec uses two parameters, <span class="math notranslate nohighlight">\( p \)</span> and <span class="math notranslate nohighlight">\( q \)</span>, to regulate the behavior of the random walk:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \mathbf{p} \)</span>: Return parameter<br />
Controls the likelihood of revisiting the previous node.</p>
<ul>
<li><p>Higher <span class="math notranslate nohighlight">\( p \)</span>: encourages staying in the local neighborhood (exploitation)</p></li>
<li><p>Lower <span class="math notranslate nohighlight">\( p \)</span>: more likely to leave the local region</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{q} \)</span>: In-out parameter<br />
Controls the likelihood of exploring distant or previously unseen nodes.</p>
<ul>
<li><p>Lower <span class="math notranslate nohighlight">\( q \)</span>: promotes exploring new regions (exploration)</p></li>
<li><p>Higher <span class="math notranslate nohighlight">\( q \)</span>: favors staying nearby</p></li>
</ul>
</li>
</ul>
<p>By tuning these parameters, Node2Vec creates <strong>flexible random walks</strong> that strike a balance between:</p>
<ul class="simple">
<li><p>üåç <strong>Global exploration</strong> of the graph (discovering community structure)</p></li>
<li><p>üèòÔ∏è <strong>Local exploitation</strong> (preserving close neighborhood features)</p></li>
</ul>
</section>
</section>
<section id="how-node2vec-learns-embeddings">
<h4>üß† How Node2Vec Learns Embeddings<a class="headerlink" href="#how-node2vec-learns-embeddings" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Random Walk Generation</strong><br />
For each node in the graph, Node2Vec generates multiple biased random walks, guided by <span class="math notranslate nohighlight">\( p \)</span> and <span class="math notranslate nohighlight">\( q\)</span>.</p></li>
<li><p><strong>Skip-Gram Training</strong><br />
These walks are treated like sentences in natural language. Each node is a ‚Äúword‚Äù, and the Skip-Gram model (with negative sampling) is trained to predict the context nodes from the walk.</p></li>
<li><p><strong>Embedding Extraction</strong><br />
After training, the weights of the hidden layer in the Skip-Gram model become the <strong>vector embeddings</strong> of the nodes.</p></li>
</ol>
<section id="example-embedding-space-and-visualization">
<h5>üìä Example: Embedding Space and Visualization<a class="headerlink" href="#example-embedding-space-and-visualization" title="Link to this heading">#</a></h5>
<p>Suppose we learn <strong>100-dimensional embeddings</strong> for each node in the graph.<br />
If we reduce these embeddings to 2 dimensions (e.g., using PCA), the relative distances between nodes in this 2D space reflect their true relationships in the original graph.</p>
<p>This way, Node2Vec <strong>preserves both structural proximity and functional similarity</strong> among nodes, even in a lower-dimensional space.</p>
<p><img alt="image.png" src="content/Chapter03/attachment:ed88df22-91f8-4fba-bb31-71bfea80c054.png" /></p>
</section>
</section>
<section id="id3">
<h4>üìå Summary<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>‚úÖ Uses Skip-Gram with Negative Sampling (SGNS)</p></li>
<li><p>üîÑ Introduces <strong>biased</strong> random walks (vs. DeepWalk‚Äôs uniform ones)</p></li>
<li><p>‚öñÔ∏è Parameters <span class="math notranslate nohighlight">\( p \)</span> and <span class="math notranslate nohighlight">\( q \)</span> balance local and global exploration</p></li>
<li><p>üîç Preserves both <strong>microscopic</strong> (local) and <strong>macroscopic</strong> (global) graph properties</p></li>
<li><p>üí° Ideal for tasks like node classification, clustering, and link prediction</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">node2vec</span><span class="w"> </span><span class="kn">import</span> <span class="n">Node2Vec</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">barbell_graph</span><span class="p">(</span><span class="n">m1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m2</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">draw_graph</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">node2vec</span> <span class="o">=</span> <span class="n">Node2Vec</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">node2vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/58b6d0827406a984ebcc2b65a60419714c40a0730f057c39387922b748650559.png" src="../../_images/58b6d0827406a984ebcc2b65a60419714c40a0730f057c39387922b748650559.png" />
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computing transition probabilities:   0%|                                                       | 0/24 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computing transition probabilities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:00&lt;00:00, 760.14it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):   0%|                                                                | 0/10 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                            | 2/10 [00:00&lt;00:01,  6.59it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                       | 3/10 [00:00&lt;00:01,  5.51it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 4/10 [00:00&lt;00:01,  5.00it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 5/10 [00:00&lt;00:01,  4.72it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 6/10 [00:01&lt;00:00,  4.42it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 7/10 [00:01&lt;00:00,  4.44it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 8/10 [00:01&lt;00:00,  4.45it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 9/10 [00:01&lt;00:00,  4.57it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02&lt;00:00,  4.64it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating walks (CPU: 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02&lt;00:00,  4.38it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fecdd1c8f3afee41333642770a926535999e82aa14d8aeabc1e911af7ff90c5a.png" src="../../_images/fecdd1c8f3afee41333642770a926535999e82aa14d8aeabc1e911af7ff90c5a.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-3.5758448   0.73252887]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="edge2vec-learning-edge-representations-from-node-embeddings">
<h2>Edge2Vec: Learning Edge Representations from Node Embeddings<a class="headerlink" href="#edge2vec-learning-edge-representations-from-node-embeddings" title="Link to this heading">#</a></h2>
<p>Unlike most graph embedding methods that focus on learning representations for <strong>nodes</strong>,<br />
<strong>Edge2Vec</strong> is a simple yet effective approach designed to generate embeddings for <strong>edges</strong> in a graph.</p>
<section id="core-idea">
<h3>üí° Core Idea<a class="headerlink" href="#core-idea" title="Link to this heading">#</a></h3>
<p>Edge2Vec builds upon the embeddings learned by algorithms like <strong>Node2Vec</strong>.<br />
Rather than training a new model from scratch, it derives the embedding of an edge based on the embeddings of the two nodes it connects.</p>
<p>Let:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(v_i \)</span> and <span class="math notranslate nohighlight">\(v_j\)</span> be two <strong>adjacent nodes</strong> in the graph.</p></li>
<li><p><span class="math notranslate nohighlight">\( f(v_i) \)</span> and <span class="math notranslate nohighlight">\( f(v_j) \)</span> be the <strong>node embeddings</strong> obtained from Node2Vec (or similar methods).</p></li>
</ul>
<p>Then, the <strong>edge embedding</strong> <span class="math notranslate nohighlight">\( f(v_i, v_j) \)</span> can be computed using <strong>simple mathematical operations</strong> between <span class="math notranslate nohighlight">\( f(v_i) \)</span> and <span class="math notranslate nohighlight">\( f(v_j) \)</span>.</p>
</section>
<section id="common-edge-embedding-operators">
<h3>üßÆ Common Edge Embedding Operators<a class="headerlink" href="#common-edge-embedding-operators" title="Link to this heading">#</a></h3>
<p>Here are some typical operators used to compute the edge representation:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operator</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Hadamard Product</p></td>
<td><p><span class="math notranslate nohighlight">\( f(v_i, v_j) = f(v_i) \cdot f(v_j) \)</span></p></td>
<td><p>Element-wise product</p></td>
</tr>
<tr class="row-odd"><td><p>Average</p></td>
<td><p><span class="math notranslate nohighlight">\( f(v_i, v_j) = \frac{f(v_i) + f(v_j)}{2} \)</span></p></td>
<td><p>Element-wise average</p></td>
</tr>
<tr class="row-even"><td><p>Weighted L1</p></td>
<td><p>$ f(v_i, v_j) =</p></td>
<td><p>f(v_i) - f(v_j)</p></td>
</tr>
<tr class="row-odd"><td><p>Weighted L2</p></td>
<td><p><span class="math notranslate nohighlight">\( f(v_i, v_j) = (f(v_i) - f(v_j))^2 \)</span></p></td>
<td><p>Squared difference (element-wise)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id4">
<h3>üß† Intuition<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>These operations encode the <strong>interaction</strong> or <strong>relationship</strong> between the connected nodes.</p></li>
<li><p>Once node embeddings are available, edge embeddings can be computed <strong>without retraining</strong> a new model.</p></li>
<li><p>These edge representations are useful for tasks such as:</p>
<ul>
<li><p><strong>Link prediction</strong></p></li>
<li><p><strong>Edge classification</strong></p></li>
<li><p><strong>Similarity computation</strong> between relationships</p></li>
</ul>
</li>
</ul>
</section>
<section id="id5">
<h3>‚úÖ Summary<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Edge2Vec</strong> is not a separate model, but a <strong>post-processing step</strong> on top of Node2Vec (or similar).</p></li>
<li><p>It enables transforming node-level embeddings into <strong>edge-level representations</strong>.</p></li>
<li><p>It relies on simple and efficient vector operations that are easy to compute and interpret.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">node2vec.edges</span><span class="w"> </span><span class="kn">import</span> <span class="n">HadamardEmbedder</span>
<span class="n">edges_embs</span> <span class="o">=</span> <span class="n">HadamardEmbedder</span><span class="p">(</span><span class="n">keyed_vectors</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">edges_embs</span><span class="p">[(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/001091b875f5713ba04e2cea07ac8440f60918d1d63fc757e6765b6c792e44e3.png" src="../../_images/001091b875f5713ba04e2cea07ac8440f60918d1d63fc757e6765b6c792e44e3.png" />
</div>
</div>
</section>
</section>
<section id="graph2vec-learning-vector-representations-of-entire-graphs">
<h2>Graph2Vec: Learning Vector Representations of Entire Graphs<a class="headerlink" href="#graph2vec-learning-vector-representations-of-entire-graphs" title="Link to this heading">#</a></h2>
<p>While previous methods such as Node2Vec and Edge2Vec focus on learning embeddings for <strong>individual nodes</strong> or <strong>edges</strong>,<br />
<strong>Graph2Vec</strong> extends this idea by generating a single embedding for the <strong>entire graph</strong>.</p>
<section id="objective">
<h3>üì¶ Objective<a class="headerlink" href="#objective" title="Link to this heading">#</a></h3>
<p>Given a <strong>collection of graphs</strong>, Graph2Vec learns a vector space where <strong>each point corresponds to one graph</strong>.<br />
This is especially useful for tasks like:</p>
<ul class="simple">
<li><p>Graph classification</p></li>
<li><p>Graph similarity search</p></li>
<li><p>Graph clustering</p></li>
</ul>
</section>
<section id="underlying-concept">
<h3>üß© Underlying Concept<a class="headerlink" href="#underlying-concept" title="Link to this heading">#</a></h3>
<p>Graph2Vec adapts the principles of <strong>Word2Vec</strong> and <strong>Doc2Vec</strong> (used in NLP) to graph structures.</p>
<ul class="simple">
<li><p>A <strong>graph</strong> is treated like a <strong>document</strong>.</p></li>
<li><p><strong>Subgraphs</strong> (usually ego-nets centered around each node) are treated as <strong>words</strong>.</p></li>
<li><p>Just as Doc2Vec learns embeddings for entire documents based on the words they contain,<br />
Graph2Vec learns embeddings for entire graphs based on their substructures.</p></li>
</ul>
</section>
<section id="algorithm-steps">
<h3>ü™ú Algorithm Steps<a class="headerlink" href="#algorithm-steps" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><h3 class="rubric" id="subgraph-generation">üîç Subgraph Generation</h3>
<p>For each graph <span class="math notranslate nohighlight">\(G\)</span> in the dataset:</p>
<ul class="simple">
<li><p>Generate a set of <strong>rooted subgraphs</strong> (e.g., <strong>ego-graphs</strong>) centered on each node.</p></li>
<li><p>These subgraphs act like ‚Äúwords‚Äù for that ‚Äúgraph document‚Äù.</p></li>
</ul>
</li>
<li><h3 class="rubric" id="doc2vec-training">üß† Doc2Vec Training</h3>
<ul class="simple">
<li><p>Treat each graph as a ‚Äúdocument‚Äù and its subgraphs as ‚Äúwords‚Äù.</p></li>
<li><p>Train a <strong>Doc2Vec</strong> model using these synthetic documents.</p></li>
<li><p>The model tries to <strong>predict subgraphs (contexts)</strong> given a graph and learn useful representations.</p></li>
</ul>
</li>
<li><h3 class="rubric" id="embedding-generation">üî¢ Embedding Generation</h3>
<ul class="simple">
<li><p>Once training is complete, the <strong>hidden representation</strong> associated with each document (graph) is extracted.</p></li>
<li><p>This becomes the <strong>graph-level embedding</strong> <span class="math notranslate nohighlight">\( f(G) \in \mathbb{R}^d\)</span>, where <span class="math notranslate nohighlight">\( d \)</span> is the embedding dimension.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id6">
<h3>üìå Summary<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Graph2Vec</strong> allows us to represent <strong>entire graphs</strong> in a continuous vector space.</p></li>
<li><p>It is based on the idea of using <strong>subgraph structures as building blocks</strong>, much like words in a document.</p></li>
<li><p>Inspired by NLP techniques, it provides a flexible and unsupervised way to learn graph-level embeddings suitable for downstream tasks.</p></li>
</ul>
<p><img alt="image.png" src="content/Chapter03/attachment:86ab289b-3b74-491f-be68-bc54aa8ea094.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">karateclub</span><span class="w"> </span><span class="kn">import</span> <span class="n">Graph2Vec</span>

<span class="n">n_graphs</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_radom</span><span class="p">():</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nx</span><span class="o">.</span><span class="n">watts_strogatz_graph</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">p</span><span class="p">),</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">p</span><span class="p">]</span>

<span class="n">Gs</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_radom</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_graphs</span><span class="p">)]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Graph2Vec</span><span class="p">(</span><span class="n">dimensions</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">wl_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Gs</span><span class="p">])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">vec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ceaef40593d74654d2a670c8ca5fd38f5bb89f340b62eac866bfbfed3581c628.png" src="../../_images/ceaef40593d74654d2a670c8ca5fd38f5bb89f340b62eac866bfbfed3581c628.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "mlgbook"
        },
        kernelOptions: {
            name: "mlgbook",
            path: "./content\Chapter03"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'mlgbook'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="00_Unsupervised_Learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 3 : Unsupervised Graph learning</p>
      </div>
    </a>
    <a class="right-next"
       href="02_Autoencoders.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">AutoEncoder</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Shallow Embedding Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-factorization">Matrix Factorization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-factorization">Graph Factorization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-representation-with-global-structure-information-graphrep">Graph Representation with Global Structure Information (GraphRep)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-and-setup">Notation and Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-k-th-order-proximity">What is the <span class="math notranslate nohighlight">\( k \)</span>th-Order Proximity?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#small-graph-example-global-structure-with-graphrep">Small Graph Example: Global Structure with GraphRep</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-structure">üîπ Graph Structure</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-adjacency-matrix-a">üîπ Step 1: Adjacency Matrix <span class="math notranslate nohighlight">\( A \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-degree-matrix-d">üîπ Step 2: Degree Matrix <span class="math notranslate nohighlight">\( D \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-compute-d-1-a">üîπ Step 3: Compute <span class="math notranslate nohighlight">\( D^{-1} A \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">üîπ Interpretation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-compute-d-1-a-2">üîπ Step 4: Compute <span class="math notranslate nohighlight">\( (D^{-1}A)^2 \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">üîπ Interpretation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-left-singular-vectors-y-s-1-truncated-to-top-2">üîπ Step 5: Left Singular Vectors <span class="math notranslate nohighlight">\(  Y_s^{(1)} \)</span> (Truncated to top-2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#y-s-2">üîπ<span class="math notranslate nohighlight">\( Y_s^{(2)} \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-embedding-construction">Final Embedding Construction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-final-embedding-y-s-y-s-1-y-s-2">üîπ step 6: Final Embedding <span class="math notranslate nohighlight">\( Y_s = [Y_s^{(1)} \ \| \ Y_s^{(2)}] \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#higher-order-proximity-preserved-embedding-hope">Higher-Order Proximity Preserved Embedding ( HOPE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Final Embedding Construction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-matrix-computation">Similarity Matrix Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#katz-similarity-in-hope">Katz Similarity in HOPE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-definition">üî∏ Mathematical Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">üî∏ Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#role-in-hope">üî∏ Role in HOPE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-beta">üî∏ Choosing <span class="math notranslate nohighlight">\( \beta \)</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram-model">Skip-Gram Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">Model Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepwalk-learning-node-embeddings-via-random-walks-and-skip-gram">DeepWalk: Learning Node Embeddings via Random Walks and Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understanding-random-walks">üîÅ Step 1: Understanding Random Walks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-applying-skip-gram-to-graph-walks">üß† Step 2: Applying Skip-Gram to Graph Walks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-extracting-node-embeddings">üì¶ Step 3: Extracting Node Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-the-deepwalk-algorithm">üß≠ Summary of the DeepWalk Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">üìå Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram-with-negative-sampling-sgns">üåê Skip-Gram with Negative Sampling (SGNS)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-strategy">üîß Training Strategy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">üß† Loss Function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">üöÄ Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#used-in">üìå Used in:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-difference-of-node2vec-from-deepwalk">üîë Key Difference of Node2Vec from DeepWalk</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#biased-random-walks-exploration-vs-exploitation">üîÑ Biased Random Walks: Exploration vs. Exploitation</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-node2vec-learns-embeddings">üß† How Node2Vec Learns Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-embedding-space-and-visualization">üìä Example: Embedding Space and Visualization</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">üìå Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#edge2vec-learning-edge-representations-from-node-embeddings">Edge2Vec: Learning Edge Representations from Node Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-idea">üí° Core Idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-edge-embedding-operators">üßÆ Common Edge Embedding Operators</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">üß† Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">‚úÖ Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph2vec-learning-vector-representations-of-entire-graphs">Graph2Vec: Learning Vector Representations of Entire Graphs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective">üì¶ Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underlying-concept">üß© Underlying Concept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-steps">ü™ú Algorithm Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">üìå Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Soheila Ashkezari-T.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>